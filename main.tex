\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,setspace,geometry}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage[shortlabels]{enumitem}
\usepackage{rotating}
\usepackage{pdflscape}
\usepackage{graphicx}
\usepackage{bbm}
\usepackage[dvipsnames]{xcolor}
\usepackage{hyperref}
\hypersetup{colorlinks=true, linkcolor= BrickRed, citecolor = BrickRed, filecolor = BrickRed, urlcolor = BrickRed, hypertexnames = true}
\usepackage[]{natbib} 
\bibpunct[:]{(}{)}{,}{a}{}{,}
\geometry{left = 1.0in,right = 1.0in,top = 1.0in,bottom = 1.0in}
\usepackage[english]{babel}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{pdfpages}
\usepackage{threeparttable}
\usepackage{lscape}
\usepackage{bm}

\renewcommand*{\epsilon}{\varepsilon}

\usepackage{setspace}
\setstretch{1.2}

\title{An MPEC Estimator for Sequential Search model}
\author{Shinji Koiso\thanks{koiso-shinji970@g.ecc.u-tokyo.ac.jp, Department of Economics, University of Tokyo}, Suguru Otani\thanks{suguru.otani@e.u-tokyo.ac.jp, Market Design Center, University of Tokyo\\Declarations of interest: none}}
\date{First version: August 29, 2024\\
Current version: \today}

\begin{document}

\maketitle

\begin{abstract}
\noindent
%150 words:
In this paper, we propose a constrained maximum likelihood estimator for sequential search models, by formulating the estimation as an MPEC (Mathematical Programming with Equilibrium Constraints) problem. 
Our approach improves the numerical accuracy and avoids introducing ad hoc components and estimation and approximation errors regarding equilibrium conditions. 
Monte Carlo simulations show that the proposed estimator gives a biased estimator.
Although the above seemingly provides pessimistic results, the MPEC approach is useful for finding candidate parameters comparable to the benchmark cheating on the ad hoc look-up table.
Instead, MPEC can generate the table constructively as the solved equilibrium constraints in estimation.
\\
%100 words AER
\textbf{Keywords}: Sequential search model, Search cost, Demand estimation, MPEC \\
\textbf{JEL code}: C50, L81, D83, M31
\end{abstract}

\section{Introduction}

The consumer search process, through which individuals gather information about potential choices, is integral to understanding actual decision-making behavior. This search process has become increasingly observable to researchers, particularly through browsing data, which allows for the observation of the options an agent considers before making a final choice. The expanded availability of consumer search data has enabled researchers to estimate structural models of consumer search behavior, as discussed in \cite{ursu2023sequential}. The current benchmark model incorporates an inner loop step to solve implicit functions related to reservation prices in search decisions, treating this as a fixed-point problemâ€”a method known for its computational complexity.

To address this challenge, we propose a novel estimator based on the Mathematical Program with Equilibrium Constraints (MPEC) approach, as advocated by \cite{su2012constrained}. MPEC is a constrained optimization problem in which the constraints include equilibrium conditions. A key advantage of the MPEC approach is that it avoids the need for iterative solutions to the fixed-point problem, a notable issue in demand estimation \citep{dube2012improving}, dynamic programming \citep{su2012constrained,egesdal2015estimating}, and misclassification models \citep{lu2014mpec}. Additionally, MPEC does not introduce the approximation and estimation errors of equilibrium constraints that are common in other approaches.

Monte Carlo simulations indicate that MPEC performs comparably, though with a slightly larger bias, relative to the most prevalent estimation method that uses an ad hoc look-up table. Instead, while the results from MPEC may initially appear less optimistic, the approach is valuable for identifying candidate parameters that are on par with the benchmark, circumventing the need for the ad hoc look-up table. MPEC can generate this table constructively by solving the equilibrium constraints during estimation. We conclude that MPEC offers a viable alternative for obtaining benchmark estimates before transitioning to more complex models, particularly when the approximation and estimation errors associated with an ad hoc look-up table are unknown to researchers.




\section{Weitzman's sequential search model}

\subsection{The Weitzman (1979) Framework}
We construct the sequential search model based on \cite{weitzman1979optimal}.
See the detail in the unified survey of \cite{ursu2023sequential} and \cite{honka2019empirical}. 
A decision maker $i$ faces a set of $\mathcal{J}=\{1,\cdots,J\}$ boxes and box $j$ gives a potential reward $u_{ij}\in \mathbb{R}$ independently drawn from a known
distribution $F_{ij}(u)$.
Opening box $j$ takes cost $c_{ij}\in \mathbb{R}^{++}$. 
An outside option is denoted as $j = 0$ with a known reward $u_{i0}$ is available at no cost.
The decision maker
opens boxes via sequential search steps and her goal is to maximize her expected reward net of total costs.

Suppose that the decision maker has opened a set \(S_{i}\) of boxes,
which revealed a maximum reward value of \(u_{i}^{*}=\max _{j \in S_{i} \cup 0} u_{i j}\), and \(\bar{S}_{i}\) unopened boxes can still be
opened. 
Her dynamic programming problem choosing to stop opening boxes, in which case she
gets payoff \(u_{i}^{*}\), or to continue opening boxes is described
by the following Bellman equation:
\begin{align}
    V\left(\bar{S}_{i}, u_{i}^{*}\right)=\max \left\{u_{i}^{*}, \max _{j \in \bar{S}_{i}}\left\{-c_{i j}+W_{j}\left(\bar{S}_{i}, u_{i}^{*}\right)\right\}\right\}
\end{align}
where \(W_{j}\left(\bar{S}_{i}, u_{i}^{*}\right)\) is the expected value of continuing to open boxes and is
defined as
\begin{align}
    W_{j}\left(\bar{S}_{i}, u_{i}^{*}\right)=V\left(\bar{S}_{i} \backslash j, u_{i}^{*}\right) \int_{-\infty}^{u_{i}^{*}} d F_{i j}(u)+\int_{u_{i}^{*}}^{\infty} V\left(\bar{S}_{i} \backslash j, u\right) d F_{i j}(u).
\end{align}
Also, the reservation utility of a product \(z_{i j}\) is the utility level defined as 
\begin{align}
    \int_{z_{i j}}^{\infty}\left(u_{i j}-z_{i j}\right) d F_{i j}\left(u_{i j}\right)=c_{i j}.
\end{align}

A set of optimal decision rules, developed by
\cite{weitzman1979optimal}, is used to characterize consumers' optimal search and choice strategies. 
The rules are as follows:
\begin{enumerate}
    \item Consumers know the true distribution(s) \(F_{i j}(u)\).
    \item Search fully reveals the utility associated with product \(j\).
    \item For each consumer \(i, u_{i j}\) is independently (across \(j\) ) drawn from \(F_{i j}(u)\).
\end{enumerate}
Then, the optimal search and choice decision
rules are expressed as follows:
\begin{enumerate}
    \item Selection Rule: The consumer searches in decreasing order of reservation utilities.
    \item Stopping Rule: Search terminates when the maximum observed utility exceeds the reservation utility of any unsearched product.
    \item Choice Rule: Once the consumer stops searching, she chooses the product with the highest observed utility among all searched options.
\end{enumerate}

\subsection{Parametrizations for Empirical Work}

Empirical economists often assume consumer $i$'s utility from product $j$ has two additively separable components:
\begin{align}
    u_{i j} & =\delta_{i j}+\varepsilon_{i j}  =\left(\xi_{i j}+\mu_{i j}\right)+\varepsilon_{i j},\\
    \quad \quad \varepsilon_{i j}&\sim_{i.i.d} N(0,\sigma_{\mu}),\quad \mu_{i j} \sim_{i.i.d} N(0,\sigma_{\varepsilon})\nonumber
\end{align}
where \(\delta_{i j}\) is utility which is known by the consumer prior to search ("pre-search
utility" in the following) and \(\varepsilon_{i j}\) is utility that is only known by the consumer
after search ("post-search taste shock" in the following). 
We assume that the pre-search utility
\(\delta_{i j}\) consists of a component \(\xi_{i j}\) that can be observed by the researcher and a pre-search taste shock \(\mu_{i j}\) that
cannot be observed by the researcher. 
According to \cite{ursu2023sequential}, we need to further normalize their variance by setting $\sigma_{\mu}= \sigma_{\varepsilon} =1$. 

Under the assumption of normally distributed post-search taste shocks, we can derive the following
expression for the reservation utility:
\begin{align}
    z_{i j}=\delta_{i j}+m\left(c_{i j}\right)=\xi_{i j}+\mu_{i j}+m\left(c_{i j}\right) 
\end{align}
where \(m\left(c_{i j} \right)\) is the implicit function that solves the following equation (see \cite{kim2010online}):
\begin{align}
    c_{i j}=\phi(m)+m \times[\Phi(m)-1] \label{eq:equilibrium_constraint}
\end{align}
with \(\phi\) and \(\Phi\) denoting the standard normal pdf and cdf, respectively. \cite{weitzman1979optimal} shows the existence and uniqueness of the solution of \eqref{eq:equilibrium_constraint}.


There are four primary methods to solve \eqref{eq:equilibrium_constraint}. The first method, proposed by \cite{kim2010online}, involves pre-computing the mapping between \( m \) and \( c \) and storing it in a look-up table. The second method, suggested by \cite{jiang2021consumer}, employs Newton's method to compute reservation utilities by iteratively improving approximations to the root of the function:

\[
    q(m) = (1-\Phi(m))\left(\frac{\phi(m)}{1-\Phi(m)} - m\right) - c = 0.
\]

The third approach, introduced by \cite{elberg2019dynamic}, uses a contraction mapping defined as:

\[
    \Gamma(m) = -c + \phi(m) + m \times \Phi(m).
\]

The fourth method, proposed by \cite{morozov2023measuring}, directly estimates \( m(c_{ij}) \).

\cite{ursu2023sequential} note that each method has its limitations: (1) the first method introduces errors due to the use of linear interpolations for search cost values that do not align with grid-point values; (2) the second and third methods avoid interpolation errors but require iterative computation of \( m \) and the establishment of a convergence threshold, which may lead to numerical errors if the threshold is too loose; and (3) the fourth method involves estimation error on \( m(c_{ij}) \). In practice, both the second and third methods tend to converge rapidly and allow researchers to set tight convergence thresholds, thereby minimizing numerical issues \citep{ursu2023sequential}. Similar challenges in demand estimation are discussed and addressed by the MPEC approach, which is introduced in the following section \citep{dube2012improving}.



\section{An MPEC estimator for the sequential search model}
As the fifth method, we propose a novel and simple estimator for the sequential search model by utilizing the Mathematical Programming with Equilibrium Constraints (MPEC) procedure advocated by \cite{su2012constrained}. 
Our setting is similar to \cite{su2012constrained} in which the likelihood function function with equilibrium constraints is constructed. 
MPEC estimator avoids iterative computations to find the fixed point by evaluating the equilibrium equations as constraints.

Define $\theta$ as a set of parameters.
Given the optimal decision rules and reservation utility formula, an MPEC estimator solves the following constrained problem:
\begin{align}
    \max_{\theta}& \sum_{i\in \mathcal{N}} \log L_{i}(\theta,(z_{ij})_{j\in \mathcal{J}},(u_{ij})_{j\in \mathcal{J}})\nonumber\\
    \text{s.t.}\quad u_{i j} & =\xi_{i j}+\mu_{i j}+\varepsilon_{i j}\label{eq:mpec_formula}\\
    z_{i j}&=\xi_{i j}+\mu_{i j}+m\left(c_{i j}\right) \nonumber\\
    c_{i j}&=\phi(m)+m \times[\Phi(m)-1] \nonumber
\end{align}
where individual likelihood $L_{i}(\theta,(z_{ij})_{j\in \mathcal{J}},(u_{ij})_{j\in \mathcal{J}})$ is derived as
\begin{align*}
    L_{i}(\theta,(z_{ij}(\theta))_{j\in \mathcal{J}}) &= \Pr (\underbrace{z_{ih} \geq \max_{k \in \mathcal{J}\setminus \{1,\cdots,h\}} z_{ik} \: \forall h \in S_i}_{\text{selection rule}}\\
    & \cap \underbrace{z_{ih} \geq \max_{k = 1}^{h-1} u_{ik} \: \forall h \in S_i \cap \max_{h \in S_i \cup \{0\}} u_{ih} \geq \max_{l \in \bar{S}_i} z_{il} }_{\text{stopping rule}}\\
    & \cap \underbrace{u_{iy_i} \geq \max_{h \in S_i \cup \{0\}} u_{ih} }_{\text{choice rule}}).
\end{align*}

A remarkable advantage of MPEC is that it does not need an ad hoc look-up table which is unknown to researchers and does not incorporate approximation and estimation error of equilibrium constraints \eqref{eq:equilibrium_constraint}, in addition to the main advantage of MPEC that it does not need to solve the fixed point problem iteratively.

\section{Monte Carlo simulation results}

For the purposes of comparison, we adhere to the parameter settings of \(\xi_{ij}\) and \(c_{ij}\) in \eqref{eq:mpec_formula} as outlined in Appendix B of \cite{ursu2023sequential}. We compare the MPEC approach with their kernel-smoothed frequency estimator, which utilizes a look-up tableâ€”a method frequently employed in empirical research.\footnote{The detailed construction is described in the Appendix.} We use the same ad hoc table as \cite{ursu2023sequential}, with a grid fineness of 0.001.

We generate 50 simulated datasets, each representing 1,000 consumers who engage in search and purchase decisions in a sequential manner across four brands and an outside option (with the mean utility of the outside option normalized to zero). The utility function consists solely of brand intercepts, specified as \((\beta_1,\beta_2,\beta_3,\beta_4) = (1.0, 0.7, 0.5, 0.3)\). The logarithm of the search cost is set to \(\log c = -3.0\). We use \(D = 100\) draws for the error terms. All estimations are initiated from a vector of zeros. Our replication code, written in Julia, is available on the authors' GitHub.


\begin{table}[!htbp]
  \begin{center}
      \caption{MPEC vs benchmark}
      \label{tb:results_MPEC_1_bias_rmse} 
      %\subfloat[scale $\sigma=20$]{\input{figuretable/target_data_bias_rmse}}
      \input{figuretable/target_data_bias_rmse}
  \end{center}
  \footnotesize
  Note: We write Julia code for both estimations for fair comparison. The benchmark results correspond with Column (4) in Table B1: Monte Carlo Simulation Results of \cite{ursu2023sequential}, replicating similar results.
\end{table} 

Table \ref{tb:results_MPEC_1_bias_rmse} presents the bias and root-mean-squared error (RMSE) of the estimated coefficients across the 50 simulated datasets. The results indicate that our MPEC estimator exhibits a positive bias and a higher RMSE compared to the benchmark. Additionally, the percentage of locally solved instances is 0.22 lower than that of the benchmark, and the computational time required is longer. While these findings might initially appear discouraging, we argue that the MPEC approach remains valuable for identifying candidate parameters that are comparable to those derived using the benchmark method, which relies on an ad hoc look-up table. Moreover, the MPEC approach can generate this table constructively by solving the equilibrium constraints during the estimation process.

\section{Conclusion}

The optimal sequential search model, based on \cite{weitzman1979optimal}, has been extensively applied in empirical research, with various approaches reviewed by \cite{ursu2023sequential}. However, it is crucial to exercise caution regarding estimation and approximation accuracy. Additionally, the most commonly used approach depends on an ad hoc look-up table, the details of which are often unknown to researchers. The impact of this ad hoc component on estimates in finite samples remains uncertain.

To address these concerns, we propose an MPEC estimator that avoids the approximation and estimation of equilibrium constraints, albeit with a slight bias and higher RMSE. Despite these seemingly pessimistic results, the MPEC approach is still useful for identifying candidate parameters that are comparable to the benchmark, which relies on the ad hoc look-up table. Furthermore, the MPEC method offers the advantage of generating the look-up table constructively through the solved equilibrium constraints during the estimation process.


\bibliographystyle{aer}
\bibliography{sequential_search}

\newpage
\appendix
\section{Appendix (for online publication)}

\subsection{Crude estimator}

Define
\begin{align}
    v_{i,1h} &= z_{ih} - \max_{k \in \mathcal{J}\setminus \{1,\cdots,h\}} z_{ik} \label{v1}\\
    v_{i,2h} &=  z_{ih} - \max_{k = 1}^{h-1} u_{ik} \label{v2} \\
    v_{i,3} &= \max_{h \in S_i \cup \{0\}} u_{ih} - \max_{l \in \bar{S}_i} z_{il} \label{v3}\\
    v_{i,4} &= u_{iy_i} - \max_{h \in S_i \cup \{0\}} u_{ih}  \label{v4}
\end{align}

\begin{itemize}
    \item[1] Take $d = (1, \cdots, D)$ sets of draws of $\mu_{ij}$ and $\epsilon_{ij}$ (each set of draws contains one draw of $\mu_{ij}$ and one draw of $\epsilon_{ij}$) for each consumer-product combination, i.e., $D\times J \times N$ sets of draws.
    \item[2] For a given guess of parameters $\bm{\theta}$, compute $u^d_{ij}$ and $z^d_{ij}$ for each set of draws $d$ and each consumer-product combination.
    \item[3] Calculate the expressions in equations \eqref{v1} to \eqref{v4} for each set of draws $d$ and each consumer. Compute the likelihood contribution for each consumer and draw:
    \begin{align*}
        L_i^d = \left[\prod_{h \in S_i} \bm{1}\{v^d_{i,1h} \geq 0\} \right] \times \left[\prod_{h \in S_i} \bm{1}\{v^d_{i,2h} \geq 0\} \right] \times  \bm{1}\{v^d_{i,3} \geq 0\}  \times  \bm{1}\{v^d_{i,4} \geq 0\}
    \end{align*}
    \item[4] Compute $L_i = \frac{1}{D}\sum_{d = 1}^D L_i^d$ for each consumer.
    \item[5] Compute $\log L = \sum_{i = 1}^N \log(L_i)$
\end{itemize}

\begin{align*}
    \max_{\theta}& \sum_{i\in \mathcal{N}} \log L_{i}(\theta,(z_{ij})_{j\in \mathcal{J}},(u_{ij})_{j\in \mathcal{J}})\nonumber\\
    \text{s.t.}\quad u_{i j} & =\xi_{i j}+\mu_{i j}+\varepsilon_{i j}\label{eq:mpec_formula}\\
    z_{i j}&=\xi_{i j}+\mu_{i j}+m\left(c_{i j}\right) \nonumber\\
    c_{i j}&=\phi(m)+m \times[\Phi(m)-1] \nonumber
\end{align*}

\subsection{Kernel estimator}
To overcome drawbacks of the crude estimator, the kernel estimator uses a smooth kernel function to obtain the log-likelihood. Specifically, we use a multivariate scaled logistic cdf as the kernel function, which leads to the following the consumer-draw-specific likelihood contribution:
\begin{align*}
    L_i^d = \frac{1}{1 + \sum_{k=1}^2 \sum_{h \in S_i} \exp (- \rho_k v^d_{i,kh}) +  + \sum_{k=1}^2 \exp (- \rho_k v^d_{i,k})},
\end{align*}
where $\rho_k$ is a scaling parameter for each condition, which needs to be chosen by the researchers. The estimation procedure to obtain $v_k$ is the same as the crude estimator.

\subsection{Memo}

\begin{table}[!htbp]
  \begin{center}
      \caption{Summary of results\textcolor{blue}{[TBA]}}
      \begin{tabular}{lcccc} \hline
   Approach &  Speed & Accuracy & Required sample size & Tuning\\ 
   \hline
   Look-up table & Good & normal & ? & a look-up table \\ 
   Contraction mapping & bad & good & ? & Tolerance \\ 
   Newton & normal & good & ? & Tolerance \\ 
   MPEC & normal & normal & ? & - \\ 
   \hline
 \end{tabular}
 \label{tb:results} 
  \end{center}
  \footnotesize
  Note: If the user incorporates the kernel estimator, additional scaling parameters are necessary to all approaches.
\end{table} 

\end{document}
